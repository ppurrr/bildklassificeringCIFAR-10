{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefebdd1-11b8-49b9-91d1-c813d53eb1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installera Pytorch i cmd: pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e66451-7d30-4fee-9f8b-87568cc8871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importera libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importera data sets\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe518e9-d991-410a-8730-29ca80927ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# förberedelse av data i form av normalisering av bilder samt omvandling till tensor\n",
    "\n",
    "# Dataklass för hantering av CIFAR-10 datasetet:\n",
    "class CustomCIFAR10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True, dataset_path='dataset/'):\n",
    "        self.rawdata = datasets.CIFAR10(root=dataset_path, train=train, download=True)\n",
    "        self.mean, self.std = 0.5, 0.5 # Normaliseringsvären\n",
    "        self.train = train\n",
    "\n",
    "# Augmentationer för träningsdata \n",
    "        if self.train:\n",
    "            self.augment = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.RandomCrop(32, padding=4)\n",
    "            ])\n",
    "        else:\n",
    "            self.augment = None\n",
    "        \n",
    "    \n",
    "#Normalisering och omvandling av bilder till tensorformat\n",
    "    def preprocess_image(self, image):\n",
    "        image = Image.fromarray(np.array(image, dtype=np.uint8)) #Omvandlar bild\n",
    "        if self.train and self.augment:\n",
    "            image = self.augment(image) # Apllicerar tränings augmentationen\n",
    "        image = np.array(image, dtype=np.float32) / 255.0 #Skalning\n",
    "        image = (image - self.mean) / self.std # Normaliserar\n",
    "        return torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) # Omformaterar till tensor\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.rawdata[index]\n",
    "        return self.preprocess_image(image), label\n",
    "\n",
    "        if self.train and self.augment:\n",
    "            image = self.augment(image)\n",
    "\n",
    "        image = self.preprocess_image(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rawdata)\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc5a700-8def-46f0-99cb-03335b418f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader som laddar data i batcher och slumpar ordning\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "class DataloaderRandom:\n",
    "    def __init__(self, dataset, batch_size=64, shuffle_data=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = list(range(len(dataset)))\n",
    "        if shuffle_data:\n",
    "            shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.indices:\n",
    "            batch.append(self.dataset[idx])\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            yield batch # Återför sista batchen om den inte är full\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.dataset) + self.batch_size -1) // self.batch_size #Antal batchar\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da12cf7-6e14-405f-8f86-fff535547db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definera labels för klasserna \n",
    "classes = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6781c6d2-7868-4daf-be99-15b84d2fd3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Träningsdata: 40000, Valideringsdata: 10000, Testdata: 10000\n"
     ]
    }
   ],
   "source": [
    "# Skapa dataset och dataloaders\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Skapar dataset\n",
    "training_dataset = CustomCIFAR10Dataset(train=True)\n",
    "testing_dataset = CustomCIFAR10Dataset(train=False)\n",
    "\n",
    "training_size = int(0.8 * len(training_dataset))\n",
    "validation_size = len(training_dataset) - training_size\n",
    "training_data, validation_data = random_split(training_dataset, [training_size, validation_size])\n",
    "\n",
    "# Skapar dataloaders\n",
    "training_loader = DataloaderRandom(training_dataset, batch_size=32, shuffle_data=True)\n",
    "testing_loader = DataloaderRandom(testing_dataset, batch_size=32, shuffle_data=False)\n",
    "validation_loader = DataloaderRandom(validation_data, batch_size=32, shuffle_data=False)\n",
    "\n",
    "print(f\"Träningsdata: {len(training_data)}, Valideringsdata: {len(validation_data)}, Testdata: {len(testing_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5958f859-970b-4db5-864a-f1c31a1415de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.9843137].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.69411767].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funktion för visualisering av bilder \n",
    "\n",
    "#Visar antal bilder i dataloader:\n",
    "def display_image(dataloader, num_images=5):\n",
    "    batch = next(iter(dataloader))\n",
    "    images, labels = zip(*batch[:num_images])\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(10, 2))\n",
    "    for i, img in enumerate(images):\n",
    "        axes[i].imshow(img.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.show\n",
    "\n",
    "#Visar exempelbilder från dataset\n",
    "display_image(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738cd48f-2561-4a1e-80dc-0819141e0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skapa Neuralnetwork och definera det för bildklassificering\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dee60f6-9951-4793-b7ff-78a4ccb980ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CNN med två konolutionslagar och 3 fullt anslutna lager\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 15, kernel_size=5, stride=1, padding=2) #definerar arktitekturen (3 input kanaler,16 feauture maps, kernel med 5x5)\n",
    "        self.conv2 = nn.Conv2d(15, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5) \n",
    "\n",
    "# Definerar framåtriktad sprigning i nätverket\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d2ab864-1bc9-4082-bfb8-1c044311ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skpar nätverksinstans och definition av förlustfunktion och optimizer\n",
    "net = ImageClassifier()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa749467-1cab-4600-8395-b78c06e4ff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0...\n",
      "Loss: Train 1.2627, Validation 1.1595\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 1...\n",
      "Loss: Train 1.2510, Validation 1.1387\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 2...\n",
      "Loss: Train 1.2541, Validation 1.1206\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 3...\n",
      "Loss: Train 1.2409, Validation 1.1390\n",
      "No improvment (1/5)\n",
      "Training Epoch 4...\n",
      "Loss: Train 1.2328, Validation 1.1066\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 5...\n",
      "Loss: Train 1.2213, Validation 1.0815\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 6...\n",
      "Loss: Train 1.2162, Validation 1.0671\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 7...\n",
      "Loss: Train 1.2069, Validation 1.0573\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 8...\n",
      "Loss: Train 1.1963, Validation 1.0506\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 9...\n",
      "Loss: Train 1.1908, Validation 1.0656\n",
      "No improvment (1/5)\n",
      "Training Epoch 10...\n",
      "Loss: Train 1.1899, Validation 1.0573\n",
      "No improvment (2/5)\n",
      "Training Epoch 11...\n",
      "Loss: Train 1.1748, Validation 1.0432\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 12...\n",
      "Loss: Train 1.1733, Validation 1.0305\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 13...\n",
      "Loss: Train 1.1657, Validation 1.0250\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 14...\n",
      "Loss: Train 1.1648, Validation 1.0253\n",
      "No improvment (1/5)\n",
      "Training Epoch 15...\n",
      "Loss: Train 1.1500, Validation 1.0026\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 16...\n",
      "Loss: Train 1.1445, Validation 1.0003\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 17...\n",
      "Loss: Train 1.1439, Validation 1.0455\n",
      "No improvment (1/5)\n",
      "Training Epoch 18...\n",
      "Loss: Train 1.1466, Validation 1.0159\n",
      "No improvment (2/5)\n",
      "Training Epoch 19...\n",
      "Loss: Train 1.1302, Validation 0.9864\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 20...\n",
      "Loss: Train 1.1327, Validation 0.9752\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 21...\n",
      "Loss: Train 1.1208, Validation 0.9880\n",
      "No improvment (1/5)\n",
      "Training Epoch 22...\n",
      "Loss: Train 1.1241, Validation 0.9855\n",
      "No improvment (2/5)\n",
      "Training Epoch 23...\n",
      "Loss: Train 1.1132, Validation 0.9701\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 24...\n",
      "Loss: Train 1.1136, Validation 0.9713\n",
      "No improvment (1/5)\n",
      "Training Epoch 25...\n",
      "Loss: Train 1.1034, Validation 0.9638\n",
      "Model saved(validation loss improved)\n",
      "Training Epoch 26...\n",
      "Loss: Train 1.1000, Validation 0.9975\n",
      "No improvment (1/5)\n",
      "Training Epoch 27...\n",
      "Loss: Train 1.0981, Validation 0.9667\n",
      "No improvment (2/5)\n",
      "Training Epoch 28...\n",
      "Loss: Train 1.0926, Validation 0.9765\n",
      "No improvment (3/5)\n",
      "Training Epoch 29...\n",
      "Loss: Train 1.0920, Validation 0.9582\n",
      "Model saved(validation loss improved)\n"
     ]
    }
   ],
   "source": [
    "# Tillämpar träningsfas följt av en valideringsfas i varje epok \n",
    "\n",
    "# Early stop-variabel\n",
    "best_validation_loss = float('inf')\n",
    "patience = 5 # Antal epoker som får gå innan vi stoppar om validation_loss inte förbättras\n",
    "counter = 0\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f'Training Epoch {epoch}...')\n",
    "\n",
    "    # Sätter modellen i träningsläge\n",
    "    net.train()\n",
    "    total_training_loss = 0.0\n",
    "\n",
    "    for batch in training_loader:\n",
    "        inputs, labels = zip(*batch)\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_training_loss += loss.item()\n",
    "\n",
    "    avg_training_loss = total_training_loss / len(training_loader)\n",
    "\n",
    "    #Sätter modellen i valideringsläge\n",
    "    net.eval()\n",
    "    total_validation_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            inputs, labels = zip(*batch)\n",
    "            inputs = torch.stack(inputs)\n",
    "            labels = torch.tensor(labels)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            total_validation_loss += loss.item()\n",
    "            \n",
    "    avg_validation_loss = total_validation_loss / len(validation_loader)\n",
    "    print(f'Loss: Train {avg_training_loss:.4f}, Validation {avg_validation_loss:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_validation_loss < best_validation_loss:\n",
    "        best_validation_loss = avg_validation_loss\n",
    "        counter = 0\n",
    "        torch.save(net.state_dict(), 'best_model.pth') #sparar bästa modellen\n",
    "        print(\"Model saved(validation loss improved)\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvment ({counter}/{patience})\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping! Training stopped.\")\n",
    "            break #avbrott av träningsloop\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15357c4d-1806-4a03-9e64-85f08d1d60c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (conv1): Conv2d(3, 15, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(15, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_model.pth') # Spara bästa modellen\n",
    "net.load_state_dict(torch.load('best_model.pth')) # Ladda den bästa modellen\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7cbdcb4-d929-4df9-9e7d-559422c2cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 68.79%\n",
      "Prediction for testbild1.jpg: bird\n",
      "Prediction for testbild2.webp: car\n"
     ]
    }
   ],
   "source": [
    "# Utvärdering av testdatan \n",
    "correct, total = 0, 0\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in testing_loader:\n",
    "        inputs, labels = zip(*batch)\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = torch.tensor(labels)\n",
    "    \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Model Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Test av modellen på enstaka bilder\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((32, 32)) #Ändrar storlek på bilder till 32x32\n",
    "    image = transforms.ToTensor()(image) #Omvandlar till tensor\n",
    "    image = (image - 0.5) / 0.5 #M Nomalisering\n",
    "    image = image.unsqueeze(0) # Lägger till batch-dimension\n",
    "    return image\n",
    "\n",
    "image_paths = ['testbild1.jpg', 'testbild2.webp']\n",
    "images = [load_image(img) for img in image_paths]\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for image, path in zip(images, image_paths):\n",
    "        output = net(image)\n",
    "        __, predicted = torch.max(output, 1)\n",
    "        print(f'Prediction for {path}: {classes[predicted.item()]}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02de94-c5d8-4c6f-b726-6e6fdcceb614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
